{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION             \n",
    "\n",
    "In order to help you with the first assignment, this file provides a general outline of your program. You will implement the details of various pieces of Python code grouped in functions. Those functions are called within the main function, at the end of this source file. Please refer to the lecture slides for the background behind this assignment. You will submit three python files (sonar.py, cat.py, digits.py) and three pickle files (sonar_model.pkl, cat_model.pkl, digits_model.pkl) which contain trained models for each tasks.\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#### I M P O R T ######################\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt #not sure I used it\n",
    "#%matplotlib inline\n",
    "import math #not sure I used it\n",
    "import os\n",
    "import pprint\n",
    "########################################\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#### FUNCTIONS #########################\n",
    "\n",
    "##### ACTIVATION FUNCTION\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "##### LOSS FUNCTION\n",
    "def lrloss(yhat, y):\n",
    "    return 0.0 if yhat==y else -1.0*(y*np.log(yhat)+(1-y)*np.log(1-yhat))\n",
    "#Corresponds to the Local vs Global Optima\n",
    "\n",
    "\n",
    "##### PREDICTION FUNCTION\n",
    "def lrpredict(self, x):\n",
    "    return 1 if self(x)>0.5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#### CLASSES #########################\n",
    "\n",
    "class Cat_Model:\n",
    "\n",
    "    def __init__(self, dimension=(64*64), weights=None, bias=None, activation=(lambda x: x), predict=None):\n",
    "#Need to check the parameters inside def\n",
    "        self._dim = dimension\n",
    "        self.w = weights or np.random.normal(size=self._dim)\n",
    "        self.w = np.array(self.w)\n",
    "        self.b = bias if bias is not None else np.random.normal()\n",
    "        self._a = activation\n",
    "        self.predict = predict.__get__(self)\n",
    "        #No __get__ \n",
    "\n",
    "    def __str__(self):\n",
    "        \n",
    "        info = \"Simple cell neuron\\n\\\n",
    "        \\tInput dimension: %d\\n\\\n",
    "        \\tBias: %f\\n\\\n",
    "        \\tWeights: %s\\n\\\n",
    "        \\tActivation: %s\" % (self._dim, self.b, self.w, self._a.__name__)\n",
    "        return info\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        #return the output of the network\n",
    "        \n",
    "        yhat = self._a(np.dot(self.w, np.array(x)) + self.b)\n",
    "        return yhat\n",
    "\n",
    "####NEED TO DO    \n",
    "    def load_model(self, file_path):\n",
    "        '''\n",
    "        open the pickle file and update the model's parameters\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def save_model(self):\n",
    "        '''\n",
    "        save your model as 'cat_model.pkl' in the local path\n",
    "        '''\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat_Trainer:\n",
    "\n",
    "    def __init__(self, dataset, model):\n",
    "        #CHECK HERE\n",
    "        self.dataset = dataset\n",
    "        self.model = model\n",
    "        self.loss = lrloss\n",
    "        #Here we need to change and put lrloss instead. Following is to what computation it corresponds (indicated in the beginning of the doc)\n",
    "\n",
    "#THERE'S NO def cost???\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        '''\n",
    "        return the accuracy on data given data iterator\n",
    "        '''\n",
    "        print(data)\n",
    "        #acc = 100*np.mean([1 if self.model.predict(x) == y else 0 for x, y in data])\n",
    "        #print(acc)\n",
    "        return 0\n",
    "\n",
    "    def train(self, lr, ne):\n",
    "        \n",
    "        #This method should:\n",
    "        #1. display initial accuracy on the training data loaded in the constructor\n",
    "        \n",
    "        print(\"training model on data...\")\n",
    "        accuracy = self.accuracy(self.dataset)\n",
    "        print(\"initial accuracy: %.3f\" % (accuracy))\n",
    "        \n",
    "        #2. update parameters of the model instance in a loop for ne epochs using lr learning rate\n",
    "        \n",
    "        for epoch in range(ne):\n",
    "            for d in self.dataset:\n",
    "                pprint.pprint(d)\n",
    "                x, y = d\n",
    "                #x = data[d][0]\n",
    "                #y = data[d][1]\n",
    "                x = np.array(x)\n",
    "                yhat = self.model(x)\n",
    "                error = y - yhat\n",
    "                self.model.w += lr*(y-yhat)*x\n",
    "                self.model.b += lr*(y-yhat)\n",
    "            accuracy = self.accuracy(d)\n",
    "            print('>epoch=%d, learning_rate=%.3f, accuracy=%.3f' % (epoch+1, lr, accuracy))  \n",
    "        \n",
    "        #3. display final accuracy\n",
    "        print(\"training complete\")\n",
    "        print(\"final accuracy: %.3f\" % (self.accuracy(self.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = '/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/'\n",
    "data_file_name = 'cat_data.pkl' \n",
    "\n",
    "\n",
    "class Cat_Data:\n",
    "    \n",
    "    #def __getitem__(self, key):\n",
    "        #return self.data\n",
    "\n",
    "    def __init__(self, relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl'):\n",
    "#???????????????????????????????        \n",
    "        #initialize self.index; \n",
    "        self.index=0 #Why is it equal to -1 in the perceptron??\n",
    "        self.relative_path=relative_path\n",
    "        self.data_file_name=data_file_name\n",
    "        \n",
    "        self.data = self.preprocess_data(self.relative_path,self.data_file_name) #Not sure \n",
    "        pprint.pprint(self.data)\n",
    "##########################\n",
    "##########################\n",
    "\n",
    "    def preprocess_data(self,relative_path,data_file_name):\n",
    "\n",
    "##################################################################################\n",
    "#Should be run only once\n",
    "########\n",
    "\n",
    "        #load and preprocess data; \n",
    "        full_path = os.path.join(relative_path,data_file_name)\n",
    "        pickle_dataset=pickle.load(open(full_path,'rb'))\n",
    "        \n",
    "        ## Split dataset into train and test set\n",
    "        train = pickle_dataset['train']\n",
    "        test = pickle_dataset['test']\n",
    "        \n",
    "        ## Add 1 if cat and 0 if no_cat\n",
    "        train_modified = [(list(features), 0) for features in train['no_cat']]+[(list(features), 1) for features in train['cat']]\n",
    "        test_modified = [(list(d), 0) for d in test['no_cat']]+[(list(d), 1) for d in test['cat']]\n",
    "\n",
    "##Description of the dataset\n",
    "        print(\"train_modified's length is: \"+str(len(train_modified)))\n",
    "        print(\"test_modified's length is: \"+str(len(test_modified)))\n",
    "        print(\"Testing for the first value of the dataset, which is y1 and has as label: \"+str(train_modified[0][1]))\n",
    "            #the [][1] will indicate the value of the image, whether the image is a cat (will return 1) or a no_cat (will return a no_cat)\n",
    "        \n",
    "        total_counter = 0\n",
    "        cat_counter = 0\n",
    "        no_cat_counter=0\n",
    "        for key, value in train_modified:\n",
    "            #print(key)\n",
    "            total_counter = total_counter+1\n",
    "    \n",
    "            #Want to know how many images are actually a 'cat' (when the value=1)\n",
    "            if value == 1:\n",
    "                cat_counter = cat_counter+1\n",
    "            else:\n",
    "                no_cat_counter= no_cat_counter+1\n",
    "    \n",
    "        print(\"Total pictures: \"+str(total_counter))\n",
    "        print(\"Amongst which \"+str(cat_counter)+\" pictures of a cat and \"+str(no_cat_counter)+\" pictures labeled as no_cat\")\n",
    "\n",
    "##########################\n",
    "##########################\n",
    "#SHOULD BE RUN FOR EVERY SINGLE IMAGE\n",
    "##########################\n",
    "\n",
    "\n",
    "        ## TURN THE IMAGES INTO 1D VECTOR\n",
    "#Good website https://cognitiveclass.ai/blog/nested-lists-multidimensional-numpy-arrays\n",
    "#https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/\n",
    "        \n",
    "        k=self.index #NOT SURE\n",
    "        \n",
    "        #### From 3D (64x64x3) to 2D (64x64x1) image\n",
    "    #To do so, need to convert the 3 color values (Red, Green and Blue) into a single value, the mean of these 3, in this case\n",
    "        train_m = np.array(train_modified)\n",
    "        \n",
    "        #Need to loop it over each image\n",
    "        final_matrix= np.zeros((len(train_m),64,64))\n",
    "        \n",
    "        #Take the average of the 3 colors, for each picture\n",
    "        for i in range(0,train_m[0][0][0].shape[0]):\n",
    "            for j in range(0,train_m[0][0][0].shape[0]):\n",
    "                final_matrix[k][i][j] = ((int(train_m[k][0][i][j][0]) + int(train_m[k][0][i][j][1]) + int(train_m[k][0][i][j][2]))/3)\n",
    "##############Need to replace k\n",
    "        \n",
    "        #### From 2D (64x64) to 1D (4096), for each image\n",
    "        #Let's turn the 2D array into a 1D array, and then add the y (train_m[][1])\n",
    "        #We want to have a (4096,1) instead of a 64x64 AND this for every image (loop for every image)\n",
    "        features = np.zeros((1,64*64))\n",
    "        \n",
    "        #print(features.shape) #(209,4096)\n",
    "\n",
    "        features[k] = np.reshape(list(final_matrix[k]),4096)\n",
    "####NEEED TO CHANGE K\n",
    "\n",
    "        #We have now obtained an array of images where each image is \n",
    "        #flattened into a single vector of 4096 values. \n",
    "        #--> Array is of shape (209,4096) called 'features'\n",
    "        \n",
    "        final_features = ([(4097,209)])\n",
    "       \n",
    "        final_features[k] = final_features.append([zip(list(features[k]), train_m[1])])\n",
    "        #final_features[k][0:4096] = list(features[k])\n",
    "        #final_features[k][4096] = train_m[k][1]\n",
    "        \n",
    "        data = final_features\n",
    "        self.final_data=data\n",
    "        #The final data input should be \n",
    "        #data=[]\n",
    "        #for k in range(0,209): \n",
    "            #data.append([( list(np.transpose(final_features[k][0:4096])), np.transpose(final_features[k][4096]) )] )\n",
    "        \n",
    "        return self.final_data\n",
    "        #shuffle the iterator\n",
    "#NOT SURE THAT S WHAT S ASKED, CHECK PROFESOR S CODE        \n",
    "        #random.shuffle(train_modified)\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def __next__(self):\n",
    "        \n",
    "    #See example code (ngram) in slides\n",
    "        idx = self.index\n",
    "        self.index +=1\n",
    "        if self.index ==209:\n",
    "            raise StopIteration\n",
    "        return self.final_data[idx] #Not sure\n",
    "        \n",
    "        \n",
    "        \n",
    "    #def _shuffle(self):\n",
    "#NEED TO CHECK WICH DATA TO SHUFFLE, PROBABLY NOT train_modified\n",
    "    \n",
    "    #shuffle the data iterator\n",
    "        #return random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_modified's length is: 209\n",
      "test_modified's length is: 50\n",
      "Testing for the first value of the dataset, which is y1 and has as label: 0\n",
      "Total pictures: 209\n",
      "Amongst which 72 pictures of a cat and 137 pictures labeled as no_cat\n",
      "[None, [<zip object at 0x114cd4608>]]\n",
      "training model on data...\n",
      "<__main__.Cat_Data object at 0x113ab2710>\n",
      "initial accuracy: 0.000\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-27eac80d9bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-27eac80d9bb3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCat_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoid\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrpredict\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# specify the necessary arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCat_Trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mne\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# experiment with learning rate and number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#As defined earlier: train(lr, ne) where lr is the learning rate and ne the number of epochs (for the gradient), tweak those values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-bd6aa49bd7a9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, lr, ne)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;31m#x = data[d][0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m#y = data[d][1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    data = Cat_Data(relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl')\n",
    "    #data=preprocess_data(relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl')\n",
    "    model = Cat_Model(dimension=(64*64), weights=None, bias=None, activation=sigmoid ,predict=lrpredict)  # specify the necessary arguments    \n",
    "    trainer = Cat_Trainer(data, model)\n",
    "    trainer.train(lr=0.5, ne=5) # experiment with learning rate and number of epochs\n",
    "    #As defined earlier: train(lr, ne) where lr is the learning rate and ne the number of epochs (for the gradient), tweak those values\n",
    "    model.save_model()\n",
    "    #print(data[0][0])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = '/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/'\n",
    "data_file_name = 'cat_data.pkl' \n",
    "\n",
    "\n",
    "def preprocess_data(relative_path, data_file_name):\n",
    "    full_path = os.path.join(relative_path,data_file_name)\n",
    "    pickle_dataset=pickle.load(open(full_path,'rb'))\n",
    "        \n",
    "## Split dataset into train and test set\n",
    "    train = pickle_dataset['train']\n",
    "    test = pickle_dataset['test']\n",
    "        \n",
    "## Add 1 if cat and 0 if no_cat\n",
    "    train_modified = [(list(d), 0) for d in train['no_cat']]+[(list(d), 1) for d in train['cat']]\n",
    "    test_modified = [(list(d), 0) for d in test['no_cat']]+[(list(d), 1) for d in test['cat']]\n",
    "\n",
    "##Description of the dataset\n",
    "    print(\"train_modified's length is: \"+str(len(train_modified)))\n",
    "    print(\"test_modified's length is: \"+str(len(test_modified)))\n",
    "    print(\"Testing for the first value of the dataset, which is y1 and has as label: \"+str(train_modified[0][1]))\n",
    "#the [][1] will indicate the value of the image, whether the image is a cat (will return 1) or a no_cat (will return a no_cat)\n",
    "        \n",
    "    total_counter = 0\n",
    "    cat_counter = 0\n",
    "    no_cat_counter=0\n",
    "    for key, value in train_modified:\n",
    "#print(key)\n",
    "        total_counter = total_counter+1\n",
    "    \n",
    "    #Want to know how many images are actually a 'cat' (when the value=1)\n",
    "        if value == 1:\n",
    "            cat_counter = cat_counter+1\n",
    "        else:\n",
    "            no_cat_counter= no_cat_counter+1\n",
    "    \n",
    "    print(\"Total pictures: \"+str(total_counter))\n",
    "    print(\"Amongst which \"+str(cat_counter)+\" pictures of a cat and \"+str(no_cat_counter)+\" pictures labeled as no_cat\")\n",
    "        \n",
    "        ## TURN THE IMAGES INTO 1D VECTOR\n",
    "#Good website https://cognitiveclass.ai/blog/nested-lists-multidimensional-numpy-arrays\n",
    "#https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/\n",
    "\n",
    "        #### From 3D (64x64x3) to 2D (64x64x1) image\n",
    "    #To do so, need to convert the 3 color values (Red, Green and Blue) into a single value, the mean of these 3, in this case\n",
    "    train_m = np.array(train_modified)\n",
    "        \n",
    "        #Need to loop it over each image\n",
    "    final_matrix= np.zeros((len(train_m),64,64))\n",
    "        \n",
    "        #Take the average of the 3 colors, for each picture\n",
    "    for k in range(0,len(train_m)):\n",
    "        for i in range(0,train_m[0][0][0].shape[0]):\n",
    "            for j in range(0,train_m[0][0][0].shape[0]):\n",
    "                final_matrix[k][i][j] = ((int(train_m[k][0][i][j][0]) + int(train_m[k][0][i][j][1]) + int(train_m[k][0][i][j][2]))/3)\n",
    "        \n",
    "        \n",
    "        #### From 2D (64x64) to 1D (4096), for each image\n",
    "        #Let's turn the 2D array into a 1D array, and then add the y (train_m[][1])\n",
    "        #We want to have a (4096,1) instead of a 64x64 AND this for every image (loop for every image)\n",
    "    features = np.zeros((len(final_matrix),(64*64)))\n",
    "        #print(features.shape) #(209,4096)\n",
    "\n",
    "    for k in range(0,len(train_m)):\n",
    "        features[k] = np.reshape(list(final_matrix[k]),(4096))\n",
    "\n",
    "        #We have now obtained an array of images where each image is \n",
    "        #flattened into a single vector of 4096 values. \n",
    "        #--> Array is of shape (209,4096) called 'features'\n",
    "        \n",
    "    final_features = np.zeros((209,4097))\n",
    "    for k in range(0,209):\n",
    "        final_features[k][0:4096] = list(features[k])\n",
    "        final_features[k][4096] = train_m[k][1]\n",
    "\n",
    "        #The final data input should be \n",
    "    data=[]\n",
    "    for k in range(0,209): \n",
    "        data.append([( list(np.transpose(final_features[k][0:4096])), np.transpose(final_features[k][4096]) )] )\n",
    "    print(data[209][1])    \n",
    "    return data\n",
    "        \n",
    "        #shuffle the iterator\n",
    "#NOT SURE THAT S WHAT S ASKED, CHECK PROFESOR S CODE        \n",
    "        #random.shuffle(train_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
