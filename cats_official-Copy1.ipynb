{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION             \n",
    "\n",
    "In order to help you with the first assignment, this file provides a general outline of your program. You will implement the details of various pieces of Python code grouped in functions. Those functions are called within the main function, at the end of this source file. Please refer to the lecture slides for the background behind this assignment. You will submit three python files (sonar.py, cat.py, digits.py) and three pickle files (sonar_model.pkl, cat_model.pkl, digits_model.pkl) which contain trained models for each tasks.\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#### I M P O R T ######################\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt #not sure I used it\n",
    "#%matplotlib inline\n",
    "import math #not sure I used it\n",
    "import os\n",
    "import pprint\n",
    "########################################\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#### FUNCTIONS #########################\n",
    "\n",
    "##### ACTIVATION FUNCTION\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "##### LOSS FUNCTION\n",
    "def lrloss(yhat, y):\n",
    "    return 0.0 if yhat==y else -1.0*(y*np.log(yhat)+(1-y)*np.log(1-yhat))\n",
    "#Corresponds to the Local vs Global Optima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#### CLASSES #########################\n",
    "\n",
    "class Cat_Model:\n",
    "\n",
    "    def __init__(self, dimension=(64*64), weights=None, bias=None, activation=(lambda x: x), predict=None):\n",
    "#Need to check the parameters inside def\n",
    "        self._dim = dimension\n",
    "        self.w = weights or np.random.normal(size=self._dim)\n",
    "        self.w = np.array(self.w)\n",
    "        self.b = bias if bias is not None else np.random.normal()\n",
    "        self._a = activation\n",
    "        self.predict = predict.__get__(self)\n",
    "        #No __get__ \n",
    "\n",
    "    ##### PREDICTION FUNCTION\n",
    "    def lrpredict(self, x):\n",
    "        return 1 if self(x)>0.5 else 0\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        \n",
    "        info = \"Simple cell neuron\\n\\\n",
    "        \\tInput dimension: %d\\n\\\n",
    "        \\tBias: %f\\n\\\n",
    "        \\tWeights: %s\\n\\\n",
    "        \\tActivation: %s\" % (self._dim, self.b, self.w, self._a.__name__)\n",
    "        return info\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        #return the output of the network\n",
    "        \n",
    "        yhat = self._a(np.dot(self.w, np.array(x)) + self.b)\n",
    "        return yhat\n",
    "\n",
    "####NEED TO DO \n",
    "#https://www.geeksforgeeks.org/saving-a-machine-learning-model/\n",
    "#Pickle string: The pickle module implements a fundamental, but powerful algorithm for serializing and de-serializing a Python object structure. \n",
    "#pickle.dump to serialize an object hierarchy, you simply use dump().\n",
    "#pickle.load to deserialize a data stream, you call the loads() function.\n",
    "    def load_model(self, file_path):\n",
    "        \n",
    "        #open the pickle file and update the model's parameters\n",
    "        #// Deserialize a model\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def save_model(self):\n",
    "        \n",
    "        #save your model as 'cat_model.pkl' in the local path\n",
    "        #// Serialize a model\n",
    "        #relative_path = '/Users/guillaumedelande/Documents/AIGroupWork/'\n",
    "        data_file_name = 'cat_data.pkl' \n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat_Trainer:\n",
    "\n",
    "    def __init__(self, dataset, model):\n",
    "        #CHECK HERE\n",
    "        self.dataset = dataset\n",
    "        self.model = model\n",
    "        self.loss = lrloss\n",
    "        #Here we need to change and put lrloss instead. Following is to what computation it corresponds (indicated in the beginning of the doc)\n",
    "\n",
    "#THERE'S NO def cost???\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        '''\n",
    "        return the accuracy on data given data iterator\n",
    "        '''\n",
    "        #print(data)\n",
    "        #acc = 100*np.mean([1 if self.model.predict(x) == y else 0 for x, y in data])\n",
    "        #print(acc)\n",
    "        return 0\n",
    "\n",
    "    def train(self, lr, ne):\n",
    "        \n",
    "        #This method should:\n",
    "        #1. display initial accuracy on the training data loaded in the constructor\n",
    "        \n",
    "        print(\"training model on data...\")\n",
    "        accuracy = self.accuracy(self.dataset)\n",
    "        print(\"initial accuracy: %.3f\" % (accuracy))\n",
    "        \n",
    "        #2. update parameters of the model instance in a loop for ne epochs using lr learning rate\n",
    "        \n",
    "        for epoch in range(ne):\n",
    "            for d in self.dataset:\n",
    "                #pprint.pprint(d)\n",
    "                #print(type(d[0]))\n",
    "                #print(d)\n",
    "                #print(d[0])\n",
    "                \n",
    "                x, y = d\n",
    "                #x = data[d][0]\n",
    "                #y = data[d][1]\n",
    "                x = np.array(x)\n",
    "                yhat = self.model(x)\n",
    "                print(yhat, self.dataset.index, \"yhat, self.dqtqset;index[index]\")\n",
    "                error = y - yhat\n",
    "                self.model.w += lr*(y-yhat)*x\n",
    "                self.model.b += lr*(y-yhat)\n",
    "            accuracy = self.accuracy(d)\n",
    "            print('>epoch=%d, learning_rate=%.3f, accuracy=%.3f' % (epoch+1, lr, accuracy))  \n",
    "        \n",
    "        #3. display final accuracy\n",
    "        print(\"training complete\")\n",
    "        print(\"final accuracy: %.3f\" % (self.accuracy(self.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### M O D I F I E D #############################\n",
    "relative_path = '/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/'\n",
    "data_file_name = 'cat_data.pkl' \n",
    "\n",
    "class Cat_Data():\n",
    "    def __init__(self, relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl'):\n",
    "        #initialize self.index; \n",
    "        self.index = 0 #index of image, or image number. \n",
    "        self.data=[] #We start with an empty dataset that will add a new image for every increment in self.index\n",
    "\n",
    "        \n",
    "        \n",
    "#load and preprocess data;\n",
    "        \n",
    "    ###Load data\n",
    "        self.relative_path=relative_path\n",
    "        self.data_file_name=data_file_name\n",
    "        \n",
    "        full_path = os.path.join(relative_path,data_file_name)\n",
    "        pickle_dataset=pickle.load(open(full_path,'rb'))\n",
    "        \n",
    "        \n",
    "    ###Preprocess data\n",
    "        ### Split dataset into train and test set\n",
    "        train = pickle_dataset['train']\n",
    "        test = pickle_dataset['test']\n",
    "        \n",
    "            ### Add 1 if cat and 0 if no_cat\n",
    "        train_modified = [(list(features), 0) for features in train['no_cat']]+[(list(features), 1) for features in train['cat']]\n",
    "        test_modified = [(list(d), 0) for d in test['no_cat']]+[(list(d), 1) for d in test['cat']]\n",
    "\n",
    "        print('---------------')\n",
    "        #print(train_modified[0]) #This prints the image number 1 with 64 lines/rows x 64 columns x 3colors\n",
    "        \n",
    "        ###Description of the dataset\n",
    "        print('------------------------')\n",
    "        print(\"train_modified's length is: \"+str(len(train_modified)))\n",
    "        print(\"test_modified's length is: \"+str(len(test_modified)))\n",
    "        print(\"Testing for the first value of the dataset, which is y1 and has as label (train_modified[0][1]): \"+str(train_modified[0][1]))\n",
    "            #the [][1] will indicate the value of the image, whether the image is a cat (will return 1) or a no_cat (will return a no_cat)\n",
    "        \n",
    "        total_counter = 0\n",
    "        cat_counter = 0\n",
    "        no_cat_counter=0\n",
    "        for key, value in train_modified:\n",
    "            #print(key)\n",
    "            total_counter = total_counter+1\n",
    "    \n",
    "            #Want to know how many images are actually a 'cat' (when the value=1)\n",
    "            if value == 1:\n",
    "                cat_counter = cat_counter+1\n",
    "            else:\n",
    "                no_cat_counter= no_cat_counter+1\n",
    "    \n",
    "        print(\"Total pictures: \"+str(total_counter))\n",
    "        print(\"Amongst which \"+str(cat_counter)+\" pictures of a cat and \"+str(no_cat_counter)+\" pictures labeled as no_cat\")\n",
    "        print('------------------------')\n",
    "\n",
    "        #######################################################################################################\n",
    "        #Will allow to see what's happening in the extract_vector function\n",
    "        self.vector = self.extract_vector(train_modified)\n",
    "        #The function extract_vector will extract a vector from train_modified, depending on the self.index that is assigned\n",
    "        #print(\"Printing self.vector to check if it corresponds to the extract_vector() applied to train_modified\"+str(self.vector))\n",
    "        #print()\n",
    "        print('----------------------------------------------------')\n",
    "        \n",
    "        \n",
    "\n",
    "        #######################################################################################################\n",
    "#############################\n",
    "##########################\n",
    "#SHOULD BE RUN FOR EVERY SINGLE IMAGE\n",
    "##########################\n",
    "\n",
    "    def extract_vector(self, train_modified):\n",
    "        ## TURN THE IMAGES INTO 1D VECTOR\n",
    "#Good website https://cognitiveclass.ai/blog/nested-lists-multidimensional-numpy-arrays\n",
    "#https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/\n",
    "        \n",
    "        print(\"An image (number 0) contained in train_modified consists of: \\n Rows: \"+str(len(train_modified[self.index][0][0]))+\"\\n Columns: \"+str(len(train_modified[0][0][0]))+\"\\n Colors (depth): \"+str(len(train_modified[0][0][0][0])))\n",
    "        print('------------------------------------------------')\n",
    "        #train_modified is a list of length 2 ()\n",
    "        #the first [] indicates the image number so it should be self.index\n",
    "        #The second ([][])  indicates the row (64 in total)\n",
    "        #The third ([][][]) indicates the line (64 in total)\n",
    "        #The fourth ([][][][]) indicates the color (3 in total)\n",
    "        \n",
    "        #The format of each column and line seems to be an array of arrays: [[a],[b],[c]]\n",
    "        \n",
    "    ### STEP 1: From 3D (64x64x3) to 2D (64x64x1) image\n",
    "    #To do so, need to convert the 3 color values (Red, Green and Blue) into a single value, the mean of these 3, in this case\n",
    "        \n",
    "        #len(train_modified[0][0][0]) is 64, as is the len(train_modified[0][0])\n",
    "        #Create a new array of size (209 obs, 64 rows, 64 columns)\n",
    "        TwoD_matrix=np.ndarray(shape=(64,64), dtype=float) #shape=(209,64,64)\n",
    "        \n",
    "        #for i in line (up to 64)\n",
    "        for i in range(0,len(train_modified[0][0])):\n",
    "            # for j in column (up to 64)\n",
    "            for j in range(0,len(train_modified[0][0][0])): \n",
    "                #Remember: self.index refers to the image's number\n",
    "                #TwoD_matrix[self.index][i][j] = ((int(train_modified[self.index][0][i][j][0]) + int(train_modified[self.index][0][i][j][1]) + int(train_modified[self.index][0][i][j][2]))/3)\n",
    "                TwoD_matrix[i][j] = ((int(train_modified[self.index][0][i][j][0]) + int(train_modified[self.index][0][i][j][1]) + int(train_modified[self.index][0][i][j][2]))/3)\n",
    "\n",
    "                \n",
    "                \n",
    "        print(TwoD_matrix[0])\n",
    "        #So for image = 0 we obtain an array of arrays ([[row1],[row2],[row3]])\n",
    "        print(\"Shape of the TwoD_matrix: \"+str(TwoD_matrix.shape))\n",
    "        print('-------------------------------------------')\n",
    "        #Need to chack if it adapts when self.index increases, maybe should use an append() fct or something?\n",
    "        #Because otherwise it will generate a new (empty) matrix at each iteration no?\n",
    "        \n",
    "        \n",
    "    ### STEP 2: From 2D (64x64) to 1D (4096), for each image\n",
    "        #Let's turn the 2D array into a 1D array, and then add the y (train_m[][1])\n",
    "        #We want to have a (4096,1) instead of a 64x64 AND this for every image (loop for every image)\n",
    "        \n",
    "        OneD_vector = np.ndarray(shape=(1,64*64), dtype=float)\n",
    "        OneD_vector = np.reshape(list(TwoD_matrix),4096)\n",
    "\n",
    "        #PREVIOUS \n",
    "        #OneD_vector[self.index] = np.reshape(list(TwoD_matrix[self.index]),4096)\n",
    "        ####\n",
    "        \n",
    "        #np.reshape(a, newshape) \n",
    "        #where a: array to be reshaped, in our case we have a list of arrays. // list of rows\n",
    "        #newshape: int or tuple of ints. If an integer, then the result will be a 1-D array of that length. One shape dimension can be -1\n",
    "        print(\"The OneD_vector looks like: \"+str(OneD_vector))\n",
    "        print(\"The OneD_vector has shape:\"+str(OneD_vector.shape))\n",
    "        print('--------------------------------------------')\n",
    "        \n",
    "    #We have now:\n",
    "    # y: The label of image[self.index] contained in train_modified[self.index][1]\n",
    "    # x: The input vector of image[self.index] contained in OneD_vector[self.index]\n",
    "        \n",
    "    ### STEP 3: make a vector containing (a list of) x, and y\n",
    "        ##Brainstorming on how to combine x and y in ([pixel1 pixel2 pixel 3], y=1 or 0)\n",
    "        #Input_vector=np.ndarray(shape=(209,2), dtype=float)\n",
    "        #Input_vector = list(zip(,train_modified))\n",
    "        #Input_vector[self.index]=(np.transpose(OneD_vector[self.index]),np.transpose(train_modified[self.index]))\n",
    "        #Input_vector[self.index]=[(list(OneD_vector[self.index]), train_modified[self.index][1])]\n",
    "            #list(OneD_vector[self.index]),train_modified[self.index])]\n",
    "        \n",
    "        Input_vector=[]\n",
    "        \n",
    "        #RESULT:\n",
    "        #Input_vector=np.array(s) #Won't it create a new matrix at each iteration? Pay attention to that in the __next__\n",
    "        \n",
    "        #Input_vector.append((list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1]))))\n",
    "        \n",
    "        ##########\n",
    "        Input_vector=[(list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1])))]\n",
    "\n",
    "        #Previous\n",
    "        #Input_vector[self.index]=[(list(np.transpose(OneD_vector[self.index])),int(np.transpose(train_modified[self.index][1])))]\n",
    "        ################\n",
    "        \n",
    "        #Input_vector=[(list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1])))]\n",
    "        \n",
    "        print(\"Uncomment the following line for an example of the vector\")\n",
    "        #print(\"Example of Input_vector (tweak the self.index in the declaration of the class): \"+str(Input_vector))\n",
    "        print('----------------------------------------------------')\n",
    "    \n",
    "    \n",
    "    ######################################################################################################    \n",
    "        #shuffle the iterator ####NOT SURE WHAT HE WANTS\n",
    "        #random.shuffle(data)\n",
    "        \n",
    "        return Input_vector\n",
    "        \n",
    "    \n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        #See example code (ngram) in lecture slides\n",
    "        \n",
    "        return self\n",
    "    \n",
    "\n",
    "    def __next__(self): #not sure we need to put Input_vector as argument?\n",
    "        #See example code (ngram) in slides\n",
    "        index=self.index\n",
    "        if self.index == 208:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        self.index+=1\n",
    "        #self.data=[]\n",
    "        #self.data = self.data.append(self.vector) \n",
    "        #Normally the self.vector = self.extract(train_modified) \n",
    "        #so for a different (incremental) self.index the output of self.extract(train_modified) should be different\n",
    "        #We want to store all those different outputs into a dataset (which is self.data) via the .append() function\n",
    "        \n",
    "        #self.vector = self.extract_vector(train_modified)\n",
    "\n",
    "        #Input_vector.append((list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1]))))\n",
    "\n",
    "        return self.vector[index] #self.data\n",
    "    \n",
    "        #returns the whole dataset when self.index == 209, // which is when all the images have been processed\n",
    "        #and added to the dataset\n",
    "    \n",
    "                #As declared in __init__ :\n",
    "        #self.vector = self.extract_vector(train_modified)\n",
    "\n",
    "\n",
    "#We have now obtained an array of images where each image is flattened into a single vector of 4096 values. \n",
    "    #final_features = ([(4097,209)])\n",
    "       \n",
    "    #    Input_vector[self.index] = Input_vector.append([zip(list(features[k]), train_m[1])])\n",
    "\n",
    "    def _shuffle(self):\n",
    "        \n",
    "        #shuffle the data iterator\n",
    "        #random.shuffle(data)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "------------------------\n",
      "train_modified's length is: 209\n",
      "test_modified's length is: 50\n",
      "Testing for the first value of the dataset, which is y1 and has as label (train_modified[0][1]): 0\n",
      "Total pictures: 209\n",
      "Amongst which 72 pictures of a cat and 137 pictures labeled as no_cat\n",
      "------------------------\n",
      "An image (number 0) contained in train_modified consists of: \n",
      " Rows: 64\n",
      " Columns: 64\n",
      " Colors (depth): 3\n",
      "------------------------------------------------\n",
      "[34.66666667 38.         40.66666667 40.66666667 42.33333333 44.33333333\n",
      " 46.66666667 49.         51.66666667 54.         55.66666667 56.33333333\n",
      " 55.33333333 54.         53.33333333 53.33333333 52.33333333 51.\n",
      " 49.66666667 49.33333333 49.33333333 48.33333333 47.33333333 45.66666667\n",
      " 45.         45.         45.66666667 45.33333333 44.33333333 43.66666667\n",
      " 43.33333333 42.33333333 41.33333333 40.         39.         38.\n",
      " 37.         35.         36.         36.33333333 36.         33.66666667\n",
      " 33.66666667 34.33333333 34.         33.66666667 31.66666667 30.66666667\n",
      " 31.         30.         29.33333333 29.66666667 29.         28.66666667\n",
      " 28.66666667 28.66666667 28.66666667 27.         26.66666667 28.66666667\n",
      " 29.         28.66666667 27.66666667 24.66666667]\n",
      "Shape of the TwoD_matrix: (64, 64)\n",
      "-------------------------------------------\n",
      "The OneD_vector looks like: [34.66666667 38.         40.66666667 ...  0.          0.\n",
      "  0.        ]\n",
      "The OneD_vector has shape:(4096,)\n",
      "--------------------------------------------\n",
      "Uncomment the following line for an example of the vector\n",
      "----------------------------------------------------\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    " #data = Cat_Data(relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl')\n",
    "    #data=preprocess_data(relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl')\n",
    "def main():\n",
    "    data = Cat_Data(relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl')\n",
    "    #print(data)\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "------------------------\n",
      "train_modified's length is: 209\n",
      "test_modified's length is: 50\n",
      "Testing for the first value of the dataset, which is y1 and has as label (train_modified[0][1]): 0\n",
      "Total pictures: 209\n",
      "Amongst which 72 pictures of a cat and 137 pictures labeled as no_cat\n",
      "------------------------\n",
      "An image (number 0) contained in train_modified consists of: \n",
      " Rows: 64\n",
      " Columns: 64\n",
      " Colors (depth): 3\n",
      "------------------------------------------------\n",
      "[34.66666667 38.         40.66666667 40.66666667 42.33333333 44.33333333\n",
      " 46.66666667 49.         51.66666667 54.         55.66666667 56.33333333\n",
      " 55.33333333 54.         53.33333333 53.33333333 52.33333333 51.\n",
      " 49.66666667 49.33333333 49.33333333 48.33333333 47.33333333 45.66666667\n",
      " 45.         45.         45.66666667 45.33333333 44.33333333 43.66666667\n",
      " 43.33333333 42.33333333 41.33333333 40.         39.         38.\n",
      " 37.         35.         36.         36.33333333 36.         33.66666667\n",
      " 33.66666667 34.33333333 34.         33.66666667 31.66666667 30.66666667\n",
      " 31.         30.         29.33333333 29.66666667 29.         28.66666667\n",
      " 28.66666667 28.66666667 28.66666667 27.         26.66666667 28.66666667\n",
      " 29.         28.66666667 27.66666667 24.66666667]\n",
      "Shape of the TwoD_matrix: (64, 64)\n",
      "-------------------------------------------\n",
      "The OneD_vector looks like: [34.66666667 38.         40.66666667 ...  0.          0.\n",
      "  0.        ]\n",
      "The OneD_vector has shape:(4096,)\n",
      "--------------------------------------------\n",
      "Uncomment the following line for an example of the vector\n",
      "----------------------------------------------------\n",
      "----------------------------------------------------\n",
      "training model on data...\n",
      "initial accuracy: 0.000\n",
      "1.0 1 yhat, self.dqtqset;index[index]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-315-27eac80d9bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-315-27eac80d9bb3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCat_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoid\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrpredict\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# specify the necessary arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCat_Trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mne\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# experiment with learning rate and number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#As defined earlier: train(lr, ne) where lr is the learning rate and ne the number of epochs (for the gradient), tweak those values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-303-e8d7a786556f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, lr, ne)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0;31m#pprint.pprint(d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m#print(type(d[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-313-4826fb124f0c>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m#Input_vector.append((list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1]))))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#self.data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m#returns the whole dataset when self.index == 209, // which is when all the images have been processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    data = Cat_Data(relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl')\n",
    "    #data=preprocess_data(relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl')\n",
    "    model = Cat_Model(dimension=(64*64), weights=None, bias=None, activation=sigmoid ,predict=lrpredict)  # specify the necessary arguments    \n",
    "    trainer = Cat_Trainer(data, model)\n",
    "    trainer.train(lr=0.5, ne=5) # experiment with learning rate and number of epochs\n",
    "    #As defined earlier: train(lr, ne) where lr is the learning rate and ne the number of epochs (for the gradient), tweak those values\n",
    "    model.save_model()\n",
    "    #print(data[0][0])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BROUILLONNNNNNN\n",
    "\n",
    "relative_path = '/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/'\n",
    "data_file_name = 'cat_data.pkl' \n",
    "\n",
    "class Cat_Data():\n",
    "    def __init__(self, relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl'):\n",
    "        #initialize self.index; \n",
    "        self.index = 0 #index of image, or image number\n",
    "        \n",
    "#load and preprocess data;\n",
    "        \n",
    "    ###Load data\n",
    "        self.relative_path=relative_path\n",
    "        self.data_file_name=data_file_name\n",
    "        \n",
    "        full_path = os.path.join(relative_path,data_file_name)\n",
    "        pickle_dataset=pickle.load(open(full_path,'rb'))\n",
    "        \n",
    "        \n",
    "    ###Preprocess data\n",
    "        ### Split dataset into train and test set\n",
    "        train = pickle_dataset['train']\n",
    "        test = pickle_dataset['test']\n",
    "        \n",
    "            ### Add 1 if cat and 0 if no_cat\n",
    "        train_modified = [(list(features), 0) for features in train['no_cat']]+[(list(features), 1) for features in train['cat']]\n",
    "        test_modified = [(list(d), 0) for d in test['no_cat']]+[(list(d), 1) for d in test['cat']]\n",
    "\n",
    "        print('---------------')\n",
    "        #print(train_modified[0]) #This prints the image number 1 with 64 lines/rows x 64 columns x 3colors\n",
    "        \n",
    "        ###Description of the dataset\n",
    "        print('------------------------')\n",
    "        print(\"train_modified's length is: \"+str(len(train_modified)))\n",
    "        print(\"test_modified's length is: \"+str(len(test_modified)))\n",
    "        print(\"Testing for the first value of the dataset, which is y1 and has as label (train_modified[0][1]): \"+str(train_modified[0][1]))\n",
    "            #the [][1] will indicate the value of the image, whether the image is a cat (will return 1) or a no_cat (will return a no_cat)\n",
    "        \n",
    "        total_counter = 0\n",
    "        cat_counter = 0\n",
    "        no_cat_counter=0\n",
    "        for key, value in train_modified:\n",
    "            #print(key)\n",
    "            total_counter = total_counter+1\n",
    "    \n",
    "            #Want to know how many images are actually a 'cat' (when the value=1)\n",
    "            if value == 1:\n",
    "                cat_counter = cat_counter+1\n",
    "            else:\n",
    "                no_cat_counter= no_cat_counter+1\n",
    "    \n",
    "        print(\"Total pictures: \"+str(total_counter))\n",
    "        print(\"Amongst which \"+str(cat_counter)+\" pictures of a cat and \"+str(no_cat_counter)+\" pictures labeled as no_cat\")\n",
    "        print('------------------------')\n",
    "\n",
    "#############################\n",
    "##########################\n",
    "#SHOULD BE RUN FOR EVERY SINGLE IMAGE\n",
    "##########################\n",
    "\n",
    "\n",
    "        ## TURN THE IMAGES INTO 1D VECTOR\n",
    "#Good website https://cognitiveclass.ai/blog/nested-lists-multidimensional-numpy-arrays\n",
    "#https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/\n",
    "        \n",
    "        print(\"An image (number 0) contained in train_modified consists of: \\n Rows: \"+str(len(train_modified[self.index][0][0]))+\"\\n Columns: \"+str(len(train_modified[0][0][0]))+\"\\n Colors (depth): \"+str(len(train_modified[0][0][0][0])))\n",
    "        print('------------------------------------------------')\n",
    "        #train_modified is a list of length 2 ()\n",
    "        #the first [] indicates the image number so it should be self.index\n",
    "        #The second ([][])  indicates the row (64 in total)\n",
    "        #The third ([][][]) indicates the line (64 in total)\n",
    "        #The fourth ([][][][]) indicates the color (3 in total)\n",
    "        \n",
    "        #The format of each column and line seems to be an array of arrays: [[a],[b],[c]]\n",
    "        \n",
    "    ### STEP 1: From 3D (64x64x3) to 2D (64x64x1) image\n",
    "    #To do so, need to convert the 3 color values (Red, Green and Blue) into a single value, the mean of these 3, in this case\n",
    "        \n",
    "        #len(train_modified[0][0][0]) is 64, as is the len(train_modified[0][0])\n",
    "        #Create a new array of size (209 obs, 64 rows, 64 columns)\n",
    "        TwoD_matrix=np.ndarray(shape=(64,64), dtype=float) #shape=(209,64,64)\n",
    "        \n",
    "        #for i in line (up to 64)\n",
    "        for i in range(0,len(train_modified[0][0])):\n",
    "            # for j in column (up to 64)\n",
    "            for j in range(0,len(train_modified[0][0][0])): \n",
    "                #Remember: self.index refers to the image's number\n",
    "                #TwoD_matrix[self.index][i][j] = ((int(train_modified[self.index][0][i][j][0]) + int(train_modified[self.index][0][i][j][1]) + int(train_modified[self.index][0][i][j][2]))/3)\n",
    "                TwoD_matrix[i][j] = ((int(train_modified[self.index][0][i][j][0]) + int(train_modified[self.index][0][i][j][1]) + int(train_modified[self.index][0][i][j][2]))/3)\n",
    "\n",
    "                \n",
    "                \n",
    "        print(TwoD_matrix[0])\n",
    "        #So for image = 0 we obtain an array of arrays ([[row1],[row2],[row3]])\n",
    "        print(\"Shape of the TwoD_matrix: \"+str(TwoD_matrix.shape))\n",
    "        print('-------------------------------------------')\n",
    "        #Need to chack if it adapts when self.index increases, maybe should use an append() fct or something?\n",
    "        #Because otherwise it will generate a new (empty) matrix at each iteration no?\n",
    "        \n",
    "        \n",
    "    ### STEP 2: From 2D (64x64) to 1D (4096), for each image\n",
    "        #Let's turn the 2D array into a 1D array, and then add the y (train_m[][1])\n",
    "        #We want to have a (4096,1) instead of a 64x64 AND this for every image (loop for every image)\n",
    "        \n",
    "        OneD_vector = np.ndarray(shape=(1,64*64), dtype=float)\n",
    "        OneD_vector = np.reshape(list(TwoD_matrix),4096)\n",
    "\n",
    "        #PREVIOUS \n",
    "        #OneD_vector[self.index] = np.reshape(list(TwoD_matrix[self.index]),4096)\n",
    "        ####\n",
    "        \n",
    "        #np.reshape(a, newshape) \n",
    "        #where a: array to be reshaped, in our case we have a list of arrays. // list of rows\n",
    "        #newshape: int or tuple of ints. If an integer, then the result will be a 1-D array of that length. One shape dimension can be -1\n",
    "        print(\"The OneD_vector looks like: \"+str(OneD_vector))\n",
    "        print(\"The OneD_vector has shape:\"+str(OneD_vector.shape))\n",
    "        print('--------------------------------------------')\n",
    "        \n",
    "    #We have now:\n",
    "    # y: The label of image[self.index] contained in train_modified[self.index][1]\n",
    "    # x: The input vector of image[self.index] contained in OneD_vector[self.index]\n",
    "        \n",
    "    ### STEP 3: make a vector containing (a list of) x, and y\n",
    "        ##Brainstorming on how to combine x and y in ([pixel1 pixel2 pixel 3], y=1 or 0)\n",
    "        #Input_vector=np.ndarray(shape=(209,2), dtype=float)\n",
    "        #Input_vector = list(zip(,train_modified))\n",
    "        #Input_vector[self.index]=(np.transpose(OneD_vector[self.index]),np.transpose(train_modified[self.index]))\n",
    "        #Input_vector[self.index]=[(list(OneD_vector[self.index]), train_modified[self.index][1])]\n",
    "            #list(OneD_vector[self.index]),train_modified[self.index])]\n",
    "        \n",
    "        Input_vector=[]\n",
    "        \n",
    "        #RESULT:\n",
    "        #Input_vector=np.array(s) #Won't it create a new matrix at each iteration? Pay attention to that in the __next__\n",
    "        \n",
    "        #Input_vector.append((list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1]))))\n",
    "        \n",
    "        ##########\n",
    "        Input_vector=[(list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1])))]\n",
    "\n",
    "        #Previous\n",
    "        #Input_vector[self.index]=[(list(np.transpose(OneD_vector[self.index])),int(np.transpose(train_modified[self.index][1])))]\n",
    "        ################\n",
    "        \n",
    "        #Input_vector=[(list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1])))]\n",
    "        \n",
    "        #print(\"Example of Input_vector (tweak the self.index in the declaration of the class): \"+str(Input_vector))\n",
    "        print('----------------------------------------------------')\n",
    "    \n",
    "    \n",
    "    ######################################################################################################    \n",
    "        #shuffle the iterator ####NOT SURE WHAT HE WANTS\n",
    "        #random.shuffle(data)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        #See example code (ngram) in lecture slides\n",
    "        \n",
    "        return self\n",
    "    \n",
    "\n",
    "    def __next__(self, Input_vector): #not sure we need to put Input_vector as argument?\n",
    "        #See example code (ngram) in slides\n",
    "        self.index +=1\n",
    "        if self.index ==209:\n",
    "            raise StopIteration\n",
    "        \n",
    "        self.data=[]\n",
    "        self.data.append(Input_vector) #Not sure about the [self.index]\n",
    "        #Input_vector.append((list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1]))))\n",
    "        return self.data\n",
    "\n",
    "#We have now obtained an array of images where each image is flattened into a single vector of 4096 values. \n",
    "    #final_features = ([(4097,209)])\n",
    "       \n",
    "    #    Input_vector[self.index] = Input_vector.append([zip(list(features[k]), train_m[1])])\n",
    "\n",
    "    def _shuffle(self):\n",
    "        \n",
    "        #shuffle the data iterator\n",
    "        #random.shuffle(data)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = '/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/'\n",
    "data_file_name = 'cat_data.pkl' \n",
    "\n",
    "\n",
    "def preprocess_data(relative_path, data_file_name):\n",
    "    full_path = os.path.join(relative_path,data_file_name)\n",
    "    pickle_dataset=pickle.load(open(full_path,'rb'))\n",
    "        \n",
    "## Split dataset into train and test set\n",
    "    train = pickle_dataset['train']\n",
    "    test = pickle_dataset['test']\n",
    "        \n",
    "## Add 1 if cat and 0 if no_cat\n",
    "    train_modified = [(list(d), 0) for d in train['no_cat']]+[(list(d), 1) for d in train['cat']]\n",
    "    test_modified = [(list(d), 0) for d in test['no_cat']]+[(list(d), 1) for d in test['cat']]\n",
    "\n",
    "##Description of the dataset\n",
    "    print(\"train_modified's length is: \"+str(len(train_modified)))\n",
    "    print(\"test_modified's length is: \"+str(len(test_modified)))\n",
    "    print(\"Testing for the first value of the dataset, which is y1 and has as label: \"+str(train_modified[0][1]))\n",
    "#the [][1] will indicate the value of the image, whether the image is a cat (will return 1) or a no_cat (will return a no_cat)\n",
    "        \n",
    "    total_counter = 0\n",
    "    cat_counter = 0\n",
    "    no_cat_counter=0\n",
    "    for key, value in train_modified:\n",
    "#print(key)\n",
    "        total_counter = total_counter+1\n",
    "    \n",
    "    #Want to know how many images are actually a 'cat' (when the value=1)\n",
    "        if value == 1:\n",
    "            cat_counter = cat_counter+1\n",
    "        else:\n",
    "            no_cat_counter= no_cat_counter+1\n",
    "    \n",
    "    print(\"Total pictures: \"+str(total_counter))\n",
    "    print(\"Amongst which \"+str(cat_counter)+\" pictures of a cat and \"+str(no_cat_counter)+\" pictures labeled as no_cat\")\n",
    "        \n",
    "        ## TURN THE IMAGES INTO 1D VECTOR\n",
    "#Good website https://cognitiveclass.ai/blog/nested-lists-multidimensional-numpy-arrays\n",
    "#https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/\n",
    "\n",
    "        #### From 3D (64x64x3) to 2D (64x64x1) image\n",
    "    #To do so, need to convert the 3 color values (Red, Green and Blue) into a single value, the mean of these 3, in this case\n",
    "    train_m = np.array(train_modified)\n",
    "        \n",
    "        #Need to loop it over each image\n",
    "    final_matrix= np.zeros((len(train_m),64,64))\n",
    "        \n",
    "        #Take the average of the 3 colors, for each picture\n",
    "    for k in range(0,len(train_m)):\n",
    "        for i in range(0,train_m[0][0][0].shape[0]):\n",
    "            for j in range(0,train_m[0][0][0].shape[0]):\n",
    "                final_matrix[k][i][j] = ((int(train_m[k][0][i][j][0]) + int(train_m[k][0][i][j][1]) + int(train_m[k][0][i][j][2]))/3)\n",
    "        \n",
    "        \n",
    "        #### From 2D (64x64) to 1D (4096), for each image\n",
    "        #Let's turn the 2D array into a 1D array, and then add the y (train_m[][1])\n",
    "        #We want to have a (4096,1) instead of a 64x64 AND this for every image (loop for every image)\n",
    "    features = np.zeros((len(final_matrix),(64*64)))\n",
    "        #print(features.shape) #(209,4096)\n",
    "\n",
    "    for k in range(0,len(train_m)):\n",
    "        features[k] = np.reshape(list(final_matrix[k]),(4096))\n",
    "\n",
    "        #We have now obtained an array of images where each image is \n",
    "        #flattened into a single vector of 4096 values. \n",
    "        #--> Array is of shape (209,4096) called 'features'\n",
    "        \n",
    "    final_features = np.zeros((209,4097))\n",
    "    for k in range(0,209):\n",
    "        final_features[k][0:4096] = list(features[k])\n",
    "        final_features[k][4096] = train_m[k][1]\n",
    "\n",
    "        #The final data input should be \n",
    "    data=[]\n",
    "    for k in range(0,209): \n",
    "        data.append([( list(np.transpose(final_features[k][0:4096])), np.transpose(final_features[k][4096]) )] )\n",
    "    print(data[209][1])    \n",
    "    return data\n",
    "        \n",
    "        #shuffle the iterator\n",
    "#NOT SURE THAT S WHAT S ASKED, CHECK PROFESOR S CODE        \n",
    "        #random.shuffle(train_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = '/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/'\n",
    "data_file_name = 'cat_data.pkl' \n",
    "\n",
    "\n",
    "class Cat_Data:\n",
    "    \n",
    "    #def __getitem__(self, key):\n",
    "        #return self.data\n",
    "\n",
    "    def __init__(self, relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl'):\n",
    "#???????????????????????????????        \n",
    "        #initialize self.index; \n",
    "        self.index=0 #Why is it equal to -1 in the perceptron??\n",
    "        self.relative_path=relative_path\n",
    "        self.data_file_name=data_file_name\n",
    "        \n",
    "        loaded_data = self.load_dataset(self.relative_path,self.data_file_name)\n",
    "        \n",
    "        self.data = self.preprocess_data(loaded_data)\n",
    "        pprint.pprint(self.data)\n",
    "\n",
    "##########################\n",
    "##################################################################################\n",
    "#Should be run only once\n",
    "########\n",
    "\n",
    "    def load_dataset(self, relative_path, data_file_name):\n",
    "    \n",
    "        #load data; \n",
    "        full_path = os.path.join(relative_path,data_file_name)\n",
    "        pickle_dataset=pickle.load(open(full_path,'rb'))\n",
    "        \n",
    "        ## Split dataset into train and test set\n",
    "        train = pickle_dataset['train']\n",
    "        test = pickle_dataset['test']\n",
    "        \n",
    "        ## Add 1 if cat and 0 if no_cat\n",
    "        train_modified = [(list(features), 0) for features in train['no_cat']]+[(list(features), 1) for features in train['cat']]\n",
    "        test_modified = [(list(d), 0) for d in test['no_cat']]+[(list(d), 1) for d in test['cat']]\n",
    "\n",
    "##Description of the dataset\n",
    "        print(\"train_modified's length is: \"+str(len(train_modified)))\n",
    "        print(\"test_modified's length is: \"+str(len(test_modified)))\n",
    "        print(\"Testing for the first value of the dataset, which is y1 and has as label: \"+str(train_modified[0][1]))\n",
    "            #the [][1] will indicate the value of the image, whether the image is a cat (will return 1) or a no_cat (will return a no_cat)\n",
    "        \n",
    "        total_counter = 0\n",
    "        cat_counter = 0\n",
    "        no_cat_counter=0\n",
    "        for key, value in train_modified:\n",
    "            #print(key)\n",
    "            total_counter = total_counter+1\n",
    "    \n",
    "            #Want to know how many images are actually a 'cat' (when the value=1)\n",
    "            if value == 1:\n",
    "                cat_counter = cat_counter+1\n",
    "            else:\n",
    "                no_cat_counter= no_cat_counter+1\n",
    "    \n",
    "        print(\"Total pictures: \"+str(total_counter))\n",
    "        print(\"Amongst which \"+str(cat_counter)+\" pictures of a cat and \"+str(no_cat_counter)+\" pictures labeled as no_cat\")\n",
    "\n",
    "##########################\n",
    "\n",
    "    def preprocess_data(self, loaded_data):\n",
    "\n",
    "##########################\n",
    "##########################\n",
    "#SHOULD BE RUN FOR EVERY SINGLE IMAGE\n",
    "##########################\n",
    "\n",
    "\n",
    "        ## TURN THE IMAGES INTO 1D VECTOR\n",
    "#Good website https://cognitiveclass.ai/blog/nested-lists-multidimensional-numpy-arrays\n",
    "#https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/\n",
    "        \n",
    "        k=self.index\n",
    "        \n",
    "        #### From 3D (64x64x3) to 2D (64x64x1) image\n",
    "    #To do so, need to convert the 3 color values (Red, Green and Blue) into a single value, the mean of these 3, in this case\n",
    "        train_m = np.array(loaded_data)\n",
    "        \n",
    "        #Need to loop it over each image\n",
    "        final_matrix= np.zeros((len(train_m),64,64))\n",
    "        \n",
    "        #Take the average of the 3 colors, for each picture\n",
    "        for i in range(0,train_m[0][0][0].shape[0]):\n",
    "            for j in range(0,train_m[0][0][0].shape[0]):\n",
    "                final_matrix[k][i][j] = ((int(train_m[k][0][i][j][0]) + int(train_m[k][0][i][j][1]) + int(train_m[k][0][i][j][2]))/3)\n",
    "##############Need to replace k\n",
    "        \n",
    "        #### From 2D (64x64) to 1D (4096), for each image\n",
    "        #Let's turn the 2D array into a 1D array, and then add the y (train_m[][1])\n",
    "        #We want to have a (4096,1) instead of a 64x64 AND this for every image (loop for every image)\n",
    "        features = np.zeros((1,64*64))\n",
    "        \n",
    "        #print(features.shape) #(209,4096)\n",
    "\n",
    "        features[k] = np.reshape(list(final_matrix[k]),4096)\n",
    "####NEEED TO CHANGE K\n",
    "\n",
    "        #We have now obtained an array of images where each image is \n",
    "        #flattened into a single vector of 4096 values. \n",
    "        #--> Array is of shape (209,4096) called 'features'\n",
    "        \n",
    "        final_features = ([(4097,209)])\n",
    "       \n",
    "        final_features[k] = final_features.append([zip(list(features[k]), train_m[1])])\n",
    "        #final_features[k][0:4096] = list(features[k])\n",
    "        #final_features[k][4096] = train_m[k][1]\n",
    "        \n",
    "        data = final_features\n",
    "        self.final_data=data\n",
    "        #The final data input should be \n",
    "        #data=[]\n",
    "        #for k in range(0,209): \n",
    "            #data.append([( list(np.transpose(final_features[k][0:4096])), np.transpose(final_features[k][4096]) )] )\n",
    "        \n",
    "        return self.final_data\n",
    "        #shuffle the iterator\n",
    "#NOT SURE THAT S WHAT S ASKED, CHECK PROFESOR S CODE        \n",
    "        #random.shuffle(train_modified)\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def __next__(self):\n",
    "        \n",
    "    #See example code (ngram) in slides\n",
    "        idx = self.index\n",
    "        self.index +=1\n",
    "        if self.index ==209:\n",
    "            raise StopIteration\n",
    "        return self.final_data[idx] #Not sure\n",
    "        \n",
    "        \n",
    "        \n",
    "    #def _shuffle(self):\n",
    "#NEED TO CHECK WICH DATA TO SHUFFLE, PROBABLY NOT train_modified\n",
    "    \n",
    "    #shuffle the data iterator\n",
    "        #return random.shuffle(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
