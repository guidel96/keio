{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION             \n",
    "\n",
    "In order to help you with the first assignment, this file provides a general outline of your program. You will implement the details of various pieces of Python code grouped in functions. Those functions are called within the main function, at the end of this source file. Please refer to the lecture slides for the background behind this assignment. You will submit three python files (sonar.py, cat.py, digits.py) and three pickle files (sonar_model.pkl, cat_model.pkl, digits_model.pkl) which contain trained models for each tasks.\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#### I M P O R T ######################\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt #not sure I used it\n",
    "#%matplotlib inline\n",
    "import math #not sure I used it\n",
    "import os\n",
    "import pprint\n",
    "########################################\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#### FUNCTIONS #########################\n",
    "\n",
    "##### ACTIVATION FUNCTION\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "##### LOSS FUNCTION\n",
    "def lrloss(yhat, y):\n",
    "    return 0.0 if yhat==y else -1.0*(y*np.log(yhat)+(1-y)*np.log(1-yhat))\n",
    "#Corresponds to the Local vs Global Optima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#### CLASSES #########################\n",
    "file_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/cat_data.pkl'\n",
    "class Cat_Model:\n",
    "\n",
    "    def __init__(self, dimension=(64*64*3), weights=None, bias=None, activation=(lambda x: x), predict=None):\n",
    "#Need to check the parameters inside def\n",
    "        self._dim = dimension\n",
    "        self.w = weights or np.random.normal(size=self._dim)\n",
    "        self.w = np.array(self.w)\n",
    "        self.b = bias if bias is not None else np.random.normal()\n",
    "        self._a = activation\n",
    "        self.predict = predict.__get__(self)\n",
    "        #No __get__ \n",
    "\n",
    "    ##### PREDICTION FUNCTION\n",
    "    def lrpredict(self, x):\n",
    "        return 1 if self(x)>0.5 else 0\n",
    "    \n",
    "    def __str__(self):\n",
    "        \n",
    "        info = \"Simple cell neuron\\n\\\n",
    "        \\tInput dimension: %d\\n\\\n",
    "        \\tBias: %f\\n\\\n",
    "        \\tWeights: %s\\n\\\n",
    "        \\tActivation: %s\" % (self._dim, self.b, self.w, self._a.__name__)\n",
    "        return info\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        #return the output of the network\n",
    "        \n",
    "        yhat = self._a(np.dot(self.w, np.array(x)) + self.b)\n",
    "        return yhat\n",
    "\n",
    "####NEED TO DO \n",
    "#https://www.geeksforgeeks.org/saving-a-machine-learning-model/\n",
    "#Pickle string: The pickle module implements a fundamental, but powerful algorithm for serializing and de-serializing a Python object structure. \n",
    "#pickle.dump to serialize an object hierarchy, you simply use dump().\n",
    "#pickle.load to deserialize a data stream, you call the loads() function.\n",
    "    def load_model(self, file_path):\n",
    "        \n",
    "        #open the pickle file and update the model's parameters\n",
    "        #// Deserialize a model\n",
    "        with open(file_path, mode='rb') as f:\n",
    "            a=pickle.load(f)\n",
    "        \n",
    "        self._dim = a._dim\n",
    "        self.w = a.w\n",
    "        self.b = a.b\n",
    "        self._a = a._a\n",
    "        \n",
    "\n",
    "    def save_model(self):\n",
    "        \n",
    "        #save your model as 'cat_model.pkl' in the local path\n",
    "        #// Serialize a model\n",
    "        #relative_path = '/Users/guillaumedelande/Documents/AIGroupWork/'\n",
    "        #f= open('/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/cat_data.pkl','rb')\n",
    "        pickle.dump(self, f)\n",
    "        f.close\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat_Trainer:\n",
    "\n",
    "    def __init__(self, dataset, model):\n",
    "        #CHECK HERE\n",
    "        self.dataset = dataset\n",
    "        self.model = model\n",
    "        self.loss = lrloss\n",
    "        #Here we need to change and put lrloss instead. Following is to what computation it corresponds (indicated in the beginning of the doc)\n",
    "\n",
    "#THERE'S NO def cost???\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        \n",
    "        #return the accuracy on data given data iterator\n",
    "        \n",
    "        acc = 100*np.mean([1 if self.model.predict(x) == y else 0 for x, y in data])\n",
    "        #print(acc)\n",
    "        return acc\n",
    "\n",
    "    def train(self, lr, ne):\n",
    "        \n",
    "        #This method should:\n",
    "        #1. display initial accuracy on the training data loaded in the constructor\n",
    "        \n",
    "        print(\"training model on data...\")\n",
    "        accuracy = self.accuracy(self.dataset)\n",
    "        print(\"initial accuracy: %.3f\" % (accuracy))\n",
    "                \n",
    "        \n",
    "        #2. update parameters of the model instance in a loop for ne epochs using lr learning rate\n",
    "        \n",
    "        for epoch in range(ne):\n",
    "            J=0\n",
    "            for d in self.dataset: #.samples               \n",
    "                #print(d[0])\n",
    "                #random.shuffle(self.dataset.samples)\n",
    "                x, y = d\n",
    "\n",
    "                x = np.array(x)\n",
    "                yhat = self.model(x)\n",
    "                #print(yhat, self.dataset.index)\n",
    "                error = y - yhat\n",
    "                self.model.w += lr*(y-yhat)*x\n",
    "                self.model.b += lr*(y-yhat)\n",
    "            accuracy = self.accuracy(self.dataset)\n",
    "            print('>epoch=%d, learning_rate=%.3f, accuracy=%.3f' % (epoch+1, lr, accuracy))  \n",
    "        \n",
    "        #3. display final accuracy\n",
    "        print(\"training complete\")\n",
    "        print(\"final accuracy: %.3f\" % (self.accuracy(self.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = '/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/'\n",
    "data_file_name = 'cat_data.pkl' \n",
    "\n",
    "\n",
    "class Cat_Data():\n",
    "    def __init__(self, relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl'):\n",
    "        \n",
    "########initialize self.index; \n",
    "        self.index = -1 #index of image, or image number. \n",
    "        \n",
    "######## Load and preprocess data;\n",
    "        \n",
    "    ###Load data\n",
    "        self.relative_path=relative_path\n",
    "        self.data_file_name=data_file_name\n",
    "        \n",
    "        full_path = os.path.join(relative_path,data_file_name)\n",
    "        cat_data = pickle.load(open(full_path,'rb'))\n",
    "        #The pickle file is now loaded\n",
    "        \n",
    "        #print(cat_data['train']['cat'].keys())\n",
    "        self.samples = [(np.reshape(vector, vector.size), 1) for vector in self.standardize(cat_data['train']['cat'])]+[(np.reshape(vector, vector.size),0) for vector in self.standardize(cat_data['train']['no_cat'])]\n",
    "        self.shuffle(self.samples)\n",
    "        \n",
    "        print(\"Length of self.samples: \"+str(len(self.samples)))\n",
    "        print(\"Value of y for self.index = 0: \"+str(self.samples[self.index][1]))\n",
    "        print(\"First value (standardized) of x in the vector where self.index=0: \"+str(self.samples[208][0][0]))\n",
    "        print(\"Note: The value is different each time because of the shuffle.\")\n",
    "        print('--------------------------------------------------------------')\n",
    "\n",
    "    def standardize (self, rgb_images):\n",
    "        mean = np.mean(rgb_images, axis=(1,2), keepdims=True)\n",
    "        std = np.std(rgb_images, axis=(1,2), keepdims=True)\n",
    "        \n",
    "        #standardized_cat=standardize(cat_data['train']['cat'])[0]\n",
    "        return (rgb_images - mean) / std\n",
    "\n",
    "    def shuffle(self, a):\n",
    "        random.shuffle(a)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.index +=1\n",
    "        if self.index == (len(self.samples)-1): #Length is 209 but we start at zero so we can go up to 208 only\n",
    "            raise StopIteration\n",
    "            #self.index -1\n",
    "        return self.samples[self.index][0], self.samples[self.index][1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of self.samples: 209\n",
      "Value of y for self.index = 0: 1\n",
      "First value (standardized) of x in the vector where self.index=0: 0.5021841661452349\n",
      "Note: The value is different each time because of the shuffle.\n",
      "--------------------------------------------------------------\n",
      "training model on data...\n",
      "initial accuracy: 48.558\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-538-b9010afd6919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-538-b9010afd6919>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCat_Trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mne\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# experiment with learning rate and number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#As defined earlier: train(lr, ne) where lr is the learning rate and ne the number of epochs (for the gradient), tweak those values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#model.save_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-530-a1c93d596701>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, lr, ne)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mJ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#.samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0;31m#print(d[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;31m#random.shuffle(self.dataset.samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-537-5c90c0a56c58>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m#self.index -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    data = Cat_Data(relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl')\n",
    "    \n",
    "    #data=preprocess_data(relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl')\n",
    "    model = Cat_Model(dimension=(64*64*3), weights=None, bias=None, activation=sigmoid ,predict=lrpredict)  # specify the necessary arguments    \n",
    "\n",
    "    trainer = Cat_Trainer(data, model)\n",
    "    trainer.train(lr=0.01, ne=500) # experiment with learning rate and number of epochs\n",
    "    #As defined earlier: train(lr, ne) where lr is the learning rate and ne the number of epochs (for the gradient), tweak those values\n",
    "    #model.save_model()\n",
    "    #print(data[0][0])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finished, no need to look further than here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of self.samples: 209\n",
      "Value of y for self.index = 0: 0\n",
      "First value (standardized) of x in the vector where self.index=0: 0.5837960854916912\n",
      "Note: The value is different each time because of the shuffle.\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    " #data = Cat_Data(relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl')\n",
    "    #data=preprocess_data(relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl')\n",
    "def main():\n",
    "    data = Cat_Data(relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl')\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### M I N E #############################\n",
    "relative_path = '/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/'\n",
    "data_file_name = 'cat_data.pkl' \n",
    "\n",
    "class Cat_Data():\n",
    "    def __init__(self, relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl'):\n",
    "        #initialize self.index; \n",
    "        self.index = 0 #index of image, or image number. \n",
    "        self.data=[] #We start with an empty dataset that will add a new image for every increment in self.index\n",
    "\n",
    "        \n",
    "        \n",
    "#load and preprocess data;\n",
    "        \n",
    "    ###Load data\n",
    "        self.relative_path=relative_path\n",
    "        self.data_file_name=data_file_name\n",
    "        \n",
    "        full_path = os.path.join(relative_path,data_file_name)\n",
    "        pickle_dataset=pickle.load(open(full_path,'rb'))\n",
    "        \n",
    "        \n",
    "    ###Preprocess data\n",
    "        ### Split dataset into train and test set\n",
    "        train = pickle_dataset['train']\n",
    "        test = pickle_dataset['test']\n",
    "        \n",
    "            ### Add 1 if cat and 0 if no_cat\n",
    "        train_modified = [(list(features), 0) for features in train['no_cat']]+[(list(features), 1) for features in train['cat']]\n",
    "        test_modified = [(list(d), 0) for d in test['no_cat']]+[(list(d), 1) for d in test['cat']]\n",
    "\n",
    "        print('---------------')\n",
    "        #print(train_modified[0]) #This prints the image number 1 with 64 lines/rows x 64 columns x 3colors\n",
    "        \n",
    "        ###Description of the dataset\n",
    "        print('------------------------')\n",
    "        print(\"train_modified's length is: \"+str(len(train_modified)))\n",
    "        print(\"test_modified's length is: \"+str(len(test_modified)))\n",
    "        print(\"Testing for the first value of the dataset, which is y1 and has as label (train_modified[0][1]): \"+str(train_modified[0][1]))\n",
    "            #the [][1] will indicate the value of the image, whether the image is a cat (will return 1) or a no_cat (will return a no_cat)\n",
    "        \n",
    "        total_counter = 0\n",
    "        cat_counter = 0\n",
    "        no_cat_counter=0\n",
    "        for key, value in train_modified:\n",
    "            #print(key)\n",
    "            total_counter = total_counter+1\n",
    "    \n",
    "            #Want to know how many images are actually a 'cat' (when the value=1)\n",
    "            if value == 1:\n",
    "                cat_counter = cat_counter+1\n",
    "            else:\n",
    "                no_cat_counter= no_cat_counter+1\n",
    "    \n",
    "        print(\"Total pictures: \"+str(total_counter))\n",
    "        print(\"Amongst which \"+str(cat_counter)+\" pictures of a cat and \"+str(no_cat_counter)+\" pictures labeled as no_cat\")\n",
    "        print('------------------------')\n",
    "\n",
    "        #######################################################################################################\n",
    "        #Will allow to see what's happening in the extract_vector function\n",
    "        self.vector = self.extract_vector(train_modified)\n",
    "        #The function extract_vector will extract a vector from train_modified, depending on the self.index that is assigned\n",
    "        #print(\"Printing self.vector to check if it corresponds to the extract_vector() applied to train_modified\"+str(self.vector))\n",
    "        #print()\n",
    "        print('----------------------------------------------------')\n",
    "        \n",
    "        \n",
    "\n",
    "        #######################################################################################################\n",
    "#############################\n",
    "##########################\n",
    "#SHOULD BE RUN FOR EVERY SINGLE IMAGE\n",
    "##########################\n",
    "\n",
    "    def extract_vector(self, train_modified):\n",
    "        ## TURN THE IMAGES INTO 1D VECTOR\n",
    "#Good website https://cognitiveclass.ai/blog/nested-lists-multidimensional-numpy-arrays\n",
    "#https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/\n",
    "        \n",
    "        print(\"An image (number 0) contained in train_modified consists of: \\n Rows: \"+str(len(train_modified[self.index][0][0]))+\"\\n Columns: \"+str(len(train_modified[0][0][0]))+\"\\n Colors (depth): \"+str(len(train_modified[0][0][0][0])))\n",
    "        print('------------------------------------------------')\n",
    "        #train_modified is a list of length 2 ()\n",
    "        #the first [] indicates the image number so it should be self.index\n",
    "        #The second ([][])  indicates the row (64 in total)\n",
    "        #The third ([][][]) indicates the line (64 in total)\n",
    "        #The fourth ([][][][]) indicates the color (3 in total)\n",
    "        \n",
    "        #The format of each column and line seems to be an array of arrays: [[a],[b],[c]]\n",
    "        \n",
    "    ### STEP 1: From 3D (64x64x3) to 2D (64x64x1) image\n",
    "    #To do so, need to convert the 3 color values (Red, Green and Blue) into a single value, the mean of these 3, in this case\n",
    "        \n",
    "        #len(train_modified[0][0][0]) is 64, as is the len(train_modified[0][0])\n",
    "        #Create a new array of size (209 obs, 64 rows, 64 columns)\n",
    "        TwoD_matrix=np.ndarray(shape=(64,64), dtype=float) #shape=(209,64,64)\n",
    "        \n",
    "        #for i in line (up to 64)\n",
    "        for i in range(0,len(train_modified[0][0])):\n",
    "            # for j in column (up to 64)\n",
    "            for j in range(0,len(train_modified[0][0][0])): \n",
    "                #Remember: self.index refers to the image's number\n",
    "                #TwoD_matrix[self.index][i][j] = ((int(train_modified[self.index][0][i][j][0]) + int(train_modified[self.index][0][i][j][1]) + int(train_modified[self.index][0][i][j][2]))/3)\n",
    "                TwoD_matrix[i][j] = ((int(train_modified[self.index][0][i][j][0]) + int(train_modified[self.index][0][i][j][1]) + int(train_modified[self.index][0][i][j][2]))/3)\n",
    "\n",
    "                \n",
    "                \n",
    "        print(TwoD_matrix[0])\n",
    "        #So for image = 0 we obtain an array of arrays ([[row1],[row2],[row3]])\n",
    "        print(\"Shape of the TwoD_matrix: \"+str(TwoD_matrix.shape))\n",
    "        print('-------------------------------------------')\n",
    "        #Need to chack if it adapts when self.index increases, maybe should use an append() fct or something?\n",
    "        #Because otherwise it will generate a new (empty) matrix at each iteration no?\n",
    "        \n",
    "        \n",
    "    ### STEP 2: From 2D (64x64) to 1D (4096), for each image\n",
    "        #Let's turn the 2D array into a 1D array, and then add the y (train_m[][1])\n",
    "        #We want to have a (4096,1) instead of a 64x64 AND this for every image (loop for every image)\n",
    "        \n",
    "        OneD_vector = np.ndarray(shape=(1,64*64), dtype=float)\n",
    "        OneD_vector = np.reshape(list(TwoD_matrix),4096)\n",
    "\n",
    "        #PREVIOUS \n",
    "        #OneD_vector[self.index] = np.reshape(list(TwoD_matrix[self.index]),4096)\n",
    "        ####\n",
    "        \n",
    "        #np.reshape(a, newshape) \n",
    "        #where a: array to be reshaped, in our case we have a list of arrays. // list of rows\n",
    "        #newshape: int or tuple of ints. If an integer, then the result will be a 1-D array of that length. One shape dimension can be -1\n",
    "        print(\"The OneD_vector looks like: \"+str(OneD_vector))\n",
    "        print(\"The OneD_vector has shape:\"+str(OneD_vector.shape))\n",
    "        print('--------------------------------------------')\n",
    "        \n",
    "    #We have now:\n",
    "    # y: The label of image[self.index] contained in train_modified[self.index][1]\n",
    "    # x: The input vector of image[self.index] contained in OneD_vector[self.index]\n",
    "        \n",
    "    ### STEP 3: make a vector containing (a list of) x, and y\n",
    "        ##Brainstorming on how to combine x and y in ([pixel1 pixel2 pixel 3], y=1 or 0)\n",
    "        #Input_vector=np.ndarray(shape=(209,2), dtype=float)\n",
    "        #Input_vector = list(zip(,train_modified))\n",
    "        #Input_vector[self.index]=(np.transpose(OneD_vector[self.index]),np.transpose(train_modified[self.index]))\n",
    "        #Input_vector[self.index]=[(list(OneD_vector[self.index]), train_modified[self.index][1])]\n",
    "            #list(OneD_vector[self.index]),train_modified[self.index])]\n",
    "        \n",
    "        Input_vector=[]\n",
    "        \n",
    "        #RESULT:\n",
    "        #Input_vector=np.array(s) #Won't it create a new matrix at each iteration? Pay attention to that in the __next__\n",
    "        \n",
    "        #Input_vector.append((list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1]))))\n",
    "        \n",
    "        ##########\n",
    "        Input_vector=[(list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1])))]\n",
    "\n",
    "        #Previous\n",
    "        #Input_vector[self.index]=[(list(np.transpose(OneD_vector[self.index])),int(np.transpose(train_modified[self.index][1])))]\n",
    "        ################\n",
    "        \n",
    "        #Input_vector=[(list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1])))]\n",
    "        \n",
    "        print(\"Uncomment the following line for an example of the vector\")\n",
    "        #print(\"Example of Input_vector (tweak the self.index in the declaration of the class): \"+str(Input_vector))\n",
    "        print('----------------------------------------------------')\n",
    "    \n",
    "    \n",
    "    ######################################################################################################    \n",
    "        #shuffle the iterator ####NOT SURE WHAT HE WANTS\n",
    "        #random.shuffle(data)\n",
    "        print(Input_vector[0][0])\n",
    "        print(Input_vector[0][1])\n",
    "        return Input_vector[0][0], Input_vector[0][1]\n",
    "        \n",
    "    \n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        #See example code (ngram) in lecture slides\n",
    "        \n",
    "        return self\n",
    "    \n",
    "\n",
    "    def __next__(self): #not sure we need to put Input_vector as argument?\n",
    "        #See example code (ngram) in slides\n",
    "        index=self.index\n",
    "        if self.index == 208:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        self.index+=1\n",
    "        #self.data=[]\n",
    "        #self.data = self.data.append(self.vector) \n",
    "        #Normally the self.vector = self.extract(train_modified) \n",
    "        #so for a different (incremental) self.index the output of self.extract(train_modified) should be different\n",
    "        #We want to store all those different outputs into a dataset (which is self.data) via the .append() function\n",
    "        \n",
    "        #self.vector = self.extract_vector(train_modified)\n",
    "\n",
    "        #Input_vector.append((list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1]))))\n",
    "\n",
    "        return self.vector #self.data\n",
    "    \n",
    "        #returns the whole dataset when self.index == 209, // which is when all the images have been processed\n",
    "        #and added to the dataset\n",
    "    \n",
    "                #As declared in __init__ :\n",
    "        #self.vector = self.extract_vector(train_modified)\n",
    "\n",
    "\n",
    "#We have now obtained an array of images where each image is flattened into a single vector of 4096 values. \n",
    "    #final_features = ([(4097,209)])\n",
    "       \n",
    "    #    Input_vector[self.index] = Input_vector.append([zip(list(features[k]), train_m[1])])\n",
    "\n",
    "    def _shuffle(self):\n",
    "        \n",
    "        #shuffle the data iterator\n",
    "        #random.shuffle(data)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BROUILLONNNNNNN\n",
    "\n",
    "relative_path = '/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/'\n",
    "data_file_name = 'cat_data.pkl' \n",
    "\n",
    "class Cat_Data():\n",
    "    def __init__(self, relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl'):\n",
    "        #initialize self.index; \n",
    "        self.index = 0 #index of image, or image number\n",
    "        \n",
    "#load and preprocess data;\n",
    "        \n",
    "    ###Load data\n",
    "        self.relative_path=relative_path\n",
    "        self.data_file_name=data_file_name\n",
    "        \n",
    "        full_path = os.path.join(relative_path,data_file_name)\n",
    "        pickle_dataset=pickle.load(open(full_path,'rb'))\n",
    "        \n",
    "        \n",
    "    ###Preprocess data\n",
    "        ### Split dataset into train and test set\n",
    "        train = pickle_dataset['train']\n",
    "        test = pickle_dataset['test']\n",
    "        \n",
    "            ### Add 1 if cat and 0 if no_cat\n",
    "        train_modified = [(list(features), 0) for features in train['no_cat']]+[(list(features), 1) for features in train['cat']]\n",
    "        test_modified = [(list(d), 0) for d in test['no_cat']]+[(list(d), 1) for d in test['cat']]\n",
    "\n",
    "        print('---------------')\n",
    "        #print(train_modified[0]) #This prints the image number 1 with 64 lines/rows x 64 columns x 3colors\n",
    "        \n",
    "        ###Description of the dataset\n",
    "        print('------------------------')\n",
    "        print(\"train_modified's length is: \"+str(len(train_modified)))\n",
    "        print(\"test_modified's length is: \"+str(len(test_modified)))\n",
    "        print(\"Testing for the first value of the dataset, which is y1 and has as label (train_modified[0][1]): \"+str(train_modified[0][1]))\n",
    "            #the [][1] will indicate the value of the image, whether the image is a cat (will return 1) or a no_cat (will return a no_cat)\n",
    "        \n",
    "        total_counter = 0\n",
    "        cat_counter = 0\n",
    "        no_cat_counter=0\n",
    "        for key, value in train_modified:\n",
    "            #print(key)\n",
    "            total_counter = total_counter+1\n",
    "    \n",
    "            #Want to know how many images are actually a 'cat' (when the value=1)\n",
    "            if value == 1:\n",
    "                cat_counter = cat_counter+1\n",
    "            else:\n",
    "                no_cat_counter= no_cat_counter+1\n",
    "    \n",
    "        print(\"Total pictures: \"+str(total_counter))\n",
    "        print(\"Amongst which \"+str(cat_counter)+\" pictures of a cat and \"+str(no_cat_counter)+\" pictures labeled as no_cat\")\n",
    "        print('------------------------')\n",
    "\n",
    "#############################\n",
    "##########################\n",
    "#SHOULD BE RUN FOR EVERY SINGLE IMAGE\n",
    "##########################\n",
    "\n",
    "\n",
    "        ## TURN THE IMAGES INTO 1D VECTOR\n",
    "#Good website https://cognitiveclass.ai/blog/nested-lists-multidimensional-numpy-arrays\n",
    "#https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/\n",
    "        \n",
    "        print(\"An image (number 0) contained in train_modified consists of: \\n Rows: \"+str(len(train_modified[self.index][0][0]))+\"\\n Columns: \"+str(len(train_modified[0][0][0]))+\"\\n Colors (depth): \"+str(len(train_modified[0][0][0][0])))\n",
    "        print('------------------------------------------------')\n",
    "        #train_modified is a list of length 2 ()\n",
    "        #the first [] indicates the image number so it should be self.index\n",
    "        #The second ([][])  indicates the row (64 in total)\n",
    "        #The third ([][][]) indicates the line (64 in total)\n",
    "        #The fourth ([][][][]) indicates the color (3 in total)\n",
    "        \n",
    "        #The format of each column and line seems to be an array of arrays: [[a],[b],[c]]\n",
    "        \n",
    "    ### STEP 1: From 3D (64x64x3) to 2D (64x64x1) image\n",
    "    #To do so, need to convert the 3 color values (Red, Green and Blue) into a single value, the mean of these 3, in this case\n",
    "        \n",
    "        #len(train_modified[0][0][0]) is 64, as is the len(train_modified[0][0])\n",
    "        #Create a new array of size (209 obs, 64 rows, 64 columns)\n",
    "        TwoD_matrix=np.ndarray(shape=(64,64), dtype=float) #shape=(209,64,64)\n",
    "        \n",
    "        #for i in line (up to 64)\n",
    "        for i in range(0,len(train_modified[0][0])):\n",
    "            # for j in column (up to 64)\n",
    "            for j in range(0,len(train_modified[0][0][0])): \n",
    "                #Remember: self.index refers to the image's number\n",
    "                #TwoD_matrix[self.index][i][j] = ((int(train_modified[self.index][0][i][j][0]) + int(train_modified[self.index][0][i][j][1]) + int(train_modified[self.index][0][i][j][2]))/3)\n",
    "                TwoD_matrix[i][j] = ((int(train_modified[self.index][0][i][j][0]) + int(train_modified[self.index][0][i][j][1]) + int(train_modified[self.index][0][i][j][2]))/3)\n",
    "\n",
    "                \n",
    "                \n",
    "        print(TwoD_matrix[0])\n",
    "        #So for image = 0 we obtain an array of arrays ([[row1],[row2],[row3]])\n",
    "        print(\"Shape of the TwoD_matrix: \"+str(TwoD_matrix.shape))\n",
    "        print('-------------------------------------------')\n",
    "        #Need to chack if it adapts when self.index increases, maybe should use an append() fct or something?\n",
    "        #Because otherwise it will generate a new (empty) matrix at each iteration no?\n",
    "        \n",
    "        \n",
    "    ### STEP 2: From 2D (64x64) to 1D (4096), for each image\n",
    "        #Let's turn the 2D array into a 1D array, and then add the y (train_m[][1])\n",
    "        #We want to have a (4096,1) instead of a 64x64 AND this for every image (loop for every image)\n",
    "        \n",
    "        OneD_vector = np.ndarray(shape=(1,64*64), dtype=float)\n",
    "        OneD_vector = np.reshape(list(TwoD_matrix),4096)\n",
    "\n",
    "        #PREVIOUS \n",
    "        #OneD_vector[self.index] = np.reshape(list(TwoD_matrix[self.index]),4096)\n",
    "        ####\n",
    "        \n",
    "        #np.reshape(a, newshape) \n",
    "        #where a: array to be reshaped, in our case we have a list of arrays. // list of rows\n",
    "        #newshape: int or tuple of ints. If an integer, then the result will be a 1-D array of that length. One shape dimension can be -1\n",
    "        print(\"The OneD_vector looks like: \"+str(OneD_vector))\n",
    "        print(\"The OneD_vector has shape:\"+str(OneD_vector.shape))\n",
    "        print('--------------------------------------------')\n",
    "        \n",
    "    #We have now:\n",
    "    # y: The label of image[self.index] contained in train_modified[self.index][1]\n",
    "    # x: The input vector of image[self.index] contained in OneD_vector[self.index]\n",
    "        \n",
    "    ### STEP 3: make a vector containing (a list of) x, and y\n",
    "        ##Brainstorming on how to combine x and y in ([pixel1 pixel2 pixel 3], y=1 or 0)\n",
    "        #Input_vector=np.ndarray(shape=(209,2), dtype=float)\n",
    "        #Input_vector = list(zip(,train_modified))\n",
    "        #Input_vector[self.index]=(np.transpose(OneD_vector[self.index]),np.transpose(train_modified[self.index]))\n",
    "        #Input_vector[self.index]=[(list(OneD_vector[self.index]), train_modified[self.index][1])]\n",
    "            #list(OneD_vector[self.index]),train_modified[self.index])]\n",
    "        \n",
    "        Input_vector=[]\n",
    "        \n",
    "        #RESULT:\n",
    "        #Input_vector=np.array(s) #Won't it create a new matrix at each iteration? Pay attention to that in the __next__\n",
    "        \n",
    "        #Input_vector.append((list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1]))))\n",
    "        \n",
    "        ##########\n",
    "        Input_vector=[(list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1])))]\n",
    "\n",
    "        #Previous\n",
    "        #Input_vector[self.index]=[(list(np.transpose(OneD_vector[self.index])),int(np.transpose(train_modified[self.index][1])))]\n",
    "        ################\n",
    "        \n",
    "        #Input_vector=[(list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1])))]\n",
    "        \n",
    "        #print(\"Example of Input_vector (tweak the self.index in the declaration of the class): \"+str(Input_vector))\n",
    "        print('----------------------------------------------------')\n",
    "    \n",
    "    \n",
    "    ######################################################################################################    \n",
    "        #shuffle the iterator ####NOT SURE WHAT HE WANTS\n",
    "        #random.shuffle(data)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        #See example code (ngram) in lecture slides\n",
    "        \n",
    "        return self\n",
    "    \n",
    "\n",
    "    def __next__(self, Input_vector): #not sure we need to put Input_vector as argument?\n",
    "        #See example code (ngram) in slides\n",
    "        self.index +=1\n",
    "        if self.index ==209:\n",
    "            raise StopIteration\n",
    "        \n",
    "        self.data=[]\n",
    "        self.data.append(Input_vector) #Not sure about the [self.index]\n",
    "        #Input_vector.append((list(np.transpose(OneD_vector)),int(np.transpose(train_modified[self.index][1]))))\n",
    "        return self.data\n",
    "\n",
    "#We have now obtained an array of images where each image is flattened into a single vector of 4096 values. \n",
    "    #final_features = ([(4097,209)])\n",
    "       \n",
    "    #    Input_vector[self.index] = Input_vector.append([zip(list(features[k]), train_m[1])])\n",
    "\n",
    "    def _shuffle(self):\n",
    "        \n",
    "        #shuffle the data iterator\n",
    "        #random.shuffle(data)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = '/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/'\n",
    "data_file_name = 'cat_data.pkl' \n",
    "\n",
    "\n",
    "def preprocess_data(relative_path, data_file_name):\n",
    "    full_path = os.path.join(relative_path,data_file_name)\n",
    "    pickle_dataset=pickle.load(open(full_path,'rb'))\n",
    "        \n",
    "## Split dataset into train and test set\n",
    "    train = pickle_dataset['train']\n",
    "    test = pickle_dataset['test']\n",
    "        \n",
    "## Add 1 if cat and 0 if no_cat\n",
    "    train_modified = [(list(d), 0) for d in train['no_cat']]+[(list(d), 1) for d in train['cat']]\n",
    "    test_modified = [(list(d), 0) for d in test['no_cat']]+[(list(d), 1) for d in test['cat']]\n",
    "\n",
    "##Description of the dataset\n",
    "    print(\"train_modified's length is: \"+str(len(train_modified)))\n",
    "    print(\"test_modified's length is: \"+str(len(test_modified)))\n",
    "    print(\"Testing for the first value of the dataset, which is y1 and has as label: \"+str(train_modified[0][1]))\n",
    "#the [][1] will indicate the value of the image, whether the image is a cat (will return 1) or a no_cat (will return a no_cat)\n",
    "        \n",
    "    total_counter = 0\n",
    "    cat_counter = 0\n",
    "    no_cat_counter=0\n",
    "    for key, value in train_modified:\n",
    "#print(key)\n",
    "        total_counter = total_counter+1\n",
    "    \n",
    "    #Want to know how many images are actually a 'cat' (when the value=1)\n",
    "        if value == 1:\n",
    "            cat_counter = cat_counter+1\n",
    "        else:\n",
    "            no_cat_counter= no_cat_counter+1\n",
    "    \n",
    "    print(\"Total pictures: \"+str(total_counter))\n",
    "    print(\"Amongst which \"+str(cat_counter)+\" pictures of a cat and \"+str(no_cat_counter)+\" pictures labeled as no_cat\")\n",
    "        \n",
    "        ## TURN THE IMAGES INTO 1D VECTOR\n",
    "#Good website https://cognitiveclass.ai/blog/nested-lists-multidimensional-numpy-arrays\n",
    "#https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/\n",
    "\n",
    "        #### From 3D (64x64x3) to 2D (64x64x1) image\n",
    "    #To do so, need to convert the 3 color values (Red, Green and Blue) into a single value, the mean of these 3, in this case\n",
    "    train_m = np.array(train_modified)\n",
    "        \n",
    "        #Need to loop it over each image\n",
    "    final_matrix= np.zeros((len(train_m),64,64))\n",
    "        \n",
    "        #Take the average of the 3 colors, for each picture\n",
    "    for k in range(0,len(train_m)):\n",
    "        for i in range(0,train_m[0][0][0].shape[0]):\n",
    "            for j in range(0,train_m[0][0][0].shape[0]):\n",
    "                final_matrix[k][i][j] = ((int(train_m[k][0][i][j][0]) + int(train_m[k][0][i][j][1]) + int(train_m[k][0][i][j][2]))/3)\n",
    "        \n",
    "        \n",
    "        #### From 2D (64x64) to 1D (4096), for each image\n",
    "        #Let's turn the 2D array into a 1D array, and then add the y (train_m[][1])\n",
    "        #We want to have a (4096,1) instead of a 64x64 AND this for every image (loop for every image)\n",
    "    features = np.zeros((len(final_matrix),(64*64)))\n",
    "        #print(features.shape) #(209,4096)\n",
    "\n",
    "    for k in range(0,len(train_m)):\n",
    "        features[k] = np.reshape(list(final_matrix[k]),(4096))\n",
    "\n",
    "        #We have now obtained an array of images where each image is \n",
    "        #flattened into a single vector of 4096 values. \n",
    "        #--> Array is of shape (209,4096) called 'features'\n",
    "        \n",
    "    final_features = np.zeros((209,4097))\n",
    "    for k in range(0,209):\n",
    "        final_features[k][0:4096] = list(features[k])\n",
    "        final_features[k][4096] = train_m[k][1]\n",
    "\n",
    "        #The final data input should be \n",
    "    data=[]\n",
    "    for k in range(0,209): \n",
    "        data.append([( list(np.transpose(final_features[k][0:4096])), np.transpose(final_features[k][4096]) )] )\n",
    "    print(data[209][1])    \n",
    "    return data\n",
    "        \n",
    "        #shuffle the iterator\n",
    "#NOT SURE THAT S WHAT S ASKED, CHECK PROFESOR S CODE        \n",
    "        #random.shuffle(train_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = '/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/'\n",
    "data_file_name = 'cat_data.pkl' \n",
    "\n",
    "\n",
    "class Cat_Data:\n",
    "    \n",
    "    #def __getitem__(self, key):\n",
    "        #return self.data\n",
    "\n",
    "    def __init__(self, relative_path='/Users/guillaumedelande/Documents/AIGroupWork/stephenfitz.keio2019aia/keio2019aia/data/assignment1/', data_file_name='cat_data.pkl'):\n",
    "#???????????????????????????????        \n",
    "        #initialize self.index; \n",
    "        self.index=0 #Why is it equal to -1 in the perceptron??\n",
    "        self.relative_path=relative_path\n",
    "        self.data_file_name=data_file_name\n",
    "        \n",
    "        loaded_data = self.load_dataset(self.relative_path,self.data_file_name)\n",
    "        \n",
    "        self.data = self.preprocess_data(loaded_data)\n",
    "        pprint.pprint(self.data)\n",
    "\n",
    "##########################\n",
    "##################################################################################\n",
    "#Should be run only once\n",
    "########\n",
    "\n",
    "    def load_dataset(self, relative_path, data_file_name):\n",
    "    \n",
    "        #load data; \n",
    "        full_path = os.path.join(relative_path,data_file_name)\n",
    "        pickle_dataset=pickle.load(open(full_path,'rb'))\n",
    "        \n",
    "        ## Split dataset into train and test set\n",
    "        train = pickle_dataset['train']\n",
    "        test = pickle_dataset['test']\n",
    "        \n",
    "        ## Add 1 if cat and 0 if no_cat\n",
    "        train_modified = [(list(features), 0) for features in train['no_cat']]+[(list(features), 1) for features in train['cat']]\n",
    "        test_modified = [(list(d), 0) for d in test['no_cat']]+[(list(d), 1) for d in test['cat']]\n",
    "\n",
    "##Description of the dataset\n",
    "        print(\"train_modified's length is: \"+str(len(train_modified)))\n",
    "        print(\"test_modified's length is: \"+str(len(test_modified)))\n",
    "        print(\"Testing for the first value of the dataset, which is y1 and has as label: \"+str(train_modified[0][1]))\n",
    "            #the [][1] will indicate the value of the image, whether the image is a cat (will return 1) or a no_cat (will return a no_cat)\n",
    "        \n",
    "        total_counter = 0\n",
    "        cat_counter = 0\n",
    "        no_cat_counter=0\n",
    "        for key, value in train_modified:\n",
    "            #print(key)\n",
    "            total_counter = total_counter+1\n",
    "    \n",
    "            #Want to know how many images are actually a 'cat' (when the value=1)\n",
    "            if value == 1:\n",
    "                cat_counter = cat_counter+1\n",
    "            else:\n",
    "                no_cat_counter= no_cat_counter+1\n",
    "    \n",
    "        print(\"Total pictures: \"+str(total_counter))\n",
    "        print(\"Amongst which \"+str(cat_counter)+\" pictures of a cat and \"+str(no_cat_counter)+\" pictures labeled as no_cat\")\n",
    "\n",
    "##########################\n",
    "\n",
    "    def preprocess_data(self, loaded_data):\n",
    "\n",
    "##########################\n",
    "##########################\n",
    "#SHOULD BE RUN FOR EVERY SINGLE IMAGE\n",
    "##########################\n",
    "\n",
    "\n",
    "        ## TURN THE IMAGES INTO 1D VECTOR\n",
    "#Good website https://cognitiveclass.ai/blog/nested-lists-multidimensional-numpy-arrays\n",
    "#https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/\n",
    "        \n",
    "        k=self.index\n",
    "        \n",
    "        #### From 3D (64x64x3) to 2D (64x64x1) image\n",
    "    #To do so, need to convert the 3 color values (Red, Green and Blue) into a single value, the mean of these 3, in this case\n",
    "        train_m = np.array(loaded_data)\n",
    "        \n",
    "        #Need to loop it over each image\n",
    "        final_matrix= np.zeros((len(train_m),64,64))\n",
    "        \n",
    "        #Take the average of the 3 colors, for each picture\n",
    "        for i in range(0,train_m[0][0][0].shape[0]):\n",
    "            for j in range(0,train_m[0][0][0].shape[0]):\n",
    "                final_matrix[k][i][j] = ((int(train_m[k][0][i][j][0]) + int(train_m[k][0][i][j][1]) + int(train_m[k][0][i][j][2]))/3)\n",
    "##############Need to replace k\n",
    "        \n",
    "        #### From 2D (64x64) to 1D (4096), for each image\n",
    "        #Let's turn the 2D array into a 1D array, and then add the y (train_m[][1])\n",
    "        #We want to have a (4096,1) instead of a 64x64 AND this for every image (loop for every image)\n",
    "        features = np.zeros((1,64*64))\n",
    "        \n",
    "        #print(features.shape) #(209,4096)\n",
    "\n",
    "        features[k] = np.reshape(list(final_matrix[k]),4096)\n",
    "####NEEED TO CHANGE K\n",
    "\n",
    "        #We have now obtained an array of images where each image is \n",
    "        #flattened into a single vector of 4096 values. \n",
    "        #--> Array is of shape (209,4096) called 'features'\n",
    "        \n",
    "        final_features = ([(4097,209)])\n",
    "       \n",
    "        final_features[k] = final_features.append([zip(list(features[k]), train_m[1])])\n",
    "        #final_features[k][0:4096] = list(features[k])\n",
    "        #final_features[k][4096] = train_m[k][1]\n",
    "        \n",
    "        data = final_features\n",
    "        self.final_data=data\n",
    "        #The final data input should be \n",
    "        #data=[]\n",
    "        #for k in range(0,209): \n",
    "            #data.append([( list(np.transpose(final_features[k][0:4096])), np.transpose(final_features[k][4096]) )] )\n",
    "        \n",
    "        return self.final_data\n",
    "        #shuffle the iterator\n",
    "#NOT SURE THAT S WHAT S ASKED, CHECK PROFESOR S CODE        \n",
    "        #random.shuffle(train_modified)\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def __next__(self):\n",
    "        \n",
    "    #See example code (ngram) in slides\n",
    "        idx = self.index\n",
    "        self.index +=1\n",
    "        if self.index ==209:\n",
    "            raise StopIteration\n",
    "        return self.final_data[idx] #Not sure\n",
    "        \n",
    "        \n",
    "        \n",
    "    #def _shuffle(self):\n",
    "#NEED TO CHECK WICH DATA TO SHUFFLE, PROBABLY NOT train_modified\n",
    "    \n",
    "    #shuffle the data iterator\n",
    "        #return random.shuffle(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
